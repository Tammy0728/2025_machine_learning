import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# ========== 超參數 ==========
np.random.seed(42)
N_TOTAL = 100000        # 總資料筆數
HIDDEN = 20            # 隱藏層神經元數
EPOCHS = 5000          # 最大疊代次數（上限）
LR = 0.3              # 學習率
BATCH = 100           # mini-batch 大小

# Early Stopping 參數
PATIENCE  = 30         # 連續多少輪沒有進步就停止
MIN_DELTA = 1e-6       # 視為「有進步」的最小改善量（避免浮點數噪音）

# ========== 目標函數 ==========
def TARGET_FUNC(x):
    return 1 / (1 + 25 * x**2)

# ========== 資料產生與切分 ==========
x_all = np.random.uniform(-1, 1, size=(N_TOTAL, 1))
y_all = TARGET_FUNC(x_all)

x_trainval, x_test, y_trainval, y_test = train_test_split(
    x_all, y_all, test_size=0.2, random_state=42
)
x_train, x_val, y_train, y_val = train_test_split(
    x_trainval, y_trainval, test_size=0.25, random_state=42
)  # 0.25 of 0.8 -> 0.2

# ========== 初始化參數 ==========
def init_params(n_in, n_hidden, n_out):
    W1 = np.random.uniform(-1, 1, size=(n_hidden, n_in)) * np.sqrt(6 / (n_in + n_hidden))
    b1 = np.zeros((n_hidden, 1))
    W2 = np.random.uniform(-1, 1, size=(n_out, n_hidden)) * np.sqrt(6 / (n_hidden + n_out))
    b2 = np.zeros((n_out, 1))
    return W1, b1, W2, b2

# ========== 激活函數 ==========
def tanh(x): return np.tanh(x)
def tanh_prime(x): return 1 - np.tanh(x) ** 2

# ========== 前向傳播 ==========
def forward(x, params):
    W1, b1, W2, b2 = params
    z1 = (x @ W1.T) + b1.T
    a1 = tanh(z1)
    z2 = (a1 @ W2.T) + b2.T
    return z2, (x, z1, a1, z2)

# ========== MSE 損失 ==========
def mse_loss(yhat, y):
    return 0.5 * np.mean((yhat - y) ** 2)

# ========== 反向傳播 ==========
def backward(yhat, y, params, cache):
    W1, b1, W2, b2 = params
    x, z1, a1, z2 = cache
    B = x.shape[0]
    dy = (yhat - y) / B
    dz2 = dy
    dW2 = dz2.T @ a1
    db2 = np.sum(dz2, axis=0, keepdims=True)
    da1 = dz2 @ W2
    dz1 = da1 * tanh_prime(z1)
    dW1 = dz1.T @ x
    db1 = np.sum(dz1, axis=0, keepdims=True)
    return dW1, db1.T, dW2, db2.T

# ========== 參數更新 ==========
def sgd_update(params, grads, lr):
    W1, b1, W2, b2 = params
    dW1, db1, dW2, db2 = grads
    return W1 - lr * dW1, b1 - lr * db1, W2 - lr * dW2, b2 - lr * db2

# ========== 訓練（含 Early Stopping） ==========
params = init_params(1, HIDDEN, 1)
train_loss_hist, val_loss_hist = [], []

best_val = np.inf
best_params = None
wait = 0
stopped_epoch = None

for epoch in range(EPOCHS):
    # --- minibatch ---
    idx = np.random.choice(x_train.shape[0], BATCH, replace=False)
    xb, yb = x_train[idx], y_train[idx]

    # --- forward / loss ---
    yhat, cache = forward(xb, params)
    loss = mse_loss(yhat, yb)
    train_loss_hist.append(loss)

    # --- backward / update ---
    grads = backward(yhat, yb, params, cache)
    params = sgd_update(params, grads, LR)

    # --- validation ---
    y_val_pred, _ = forward(x_val, params)
    val_loss = mse_loss(y_val_pred, y_val)
    val_loss_hist.append(val_loss)

    # --- early stopping 檢查 ---
    if val_loss < best_val - MIN_DELTA:
        best_val = val_loss
        # 深拷貝參數（避免後續被覆蓋）
        best_params = tuple(p.copy() for p in params)
        wait = 0
    else:
        wait += 1
        if wait >= PATIENCE:
            stopped_epoch = epoch + 1
            print(f"[Early Stopping] No improvement for {PATIENCE} epochs. Stop at epoch {stopped_epoch}.")
            break

# 還原到最佳參數（若有觸發或即使未觸發也保險）
if best_params is not None:
    params = best_params

# ========== Test 評估 ==========
y_test_pred, _ = forward(x_test, params)
test_mse = mse_loss(y_test_pred, y_test)

# ========== 視覺化：Loss ==========
plt.figure(figsize=(6, 4))
plt.plot(train_loss_hist, label="Train Loss")
plt.plot(val_loss_hist, label="Validation Loss")
title = "Training vs Validation Loss "
if stopped_epoch is not None:
    title += f"\nStopped at epoch {stopped_epoch}, best Val = {best_val:.6f}"
plt.title(title)
plt.xlabel("Iteration")
plt.ylabel("Loss (MSE)")
plt.legend()
plt.grid(True)
plt.show()

# ========== 視覺化：Test Set 預測 ==========
plt.figure(figsize=(6, 4))
sorted_idx = np.argsort(x_test[:, 0])
plt.plot(x_test[sorted_idx], y_test[sorted_idx], label="Target 1/(1+25x^2)")
plt.plot(x_test[sorted_idx], y_test_pred[sorted_idx], label="NN Approximation")
plt.scatter(x_train, y_train, s=10, alpha=0.2, label="Train Data")
plt.xlabel("x")
plt.ylabel("y")
plt.title(f"Function Approximation on Test Set\n(Test MSE ≈ {test_mse:.4f})")
plt.legend()
plt.grid(True)
plt.show()

print(f"Best Validation Loss: {best_val:.6f}")
print(f"Test Loss (MSE): {test_mse:.6f}")